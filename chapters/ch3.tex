\section{Partial Derivatives}
\textit{Chapter 14: Page 888}
\subsection{Functions of Several Variables}
In this section we will study functions of multiple variables from several points of view:
\begin{enumerate}
    \item verbally 
    \item numerically
    \item algebraically
    \item visually
\end{enumerate}
\subsubsection{Functions of Two Variables}
The temperature \(T\) at a point on the surface of the Earth at a given time depends on the longitude \(x\) and latitude \(y\) of the point. We can think of \(T\) as being a function of the two variables \(x\) or \(y\), or a function of the pair \((x,y)\). We indicate this by writing \(T = f(x, y)\), or with \(f: \RR^2\to \RR\), with the \(\to\) ("to") operator indicating that \(f\) transforms from the domain of \(\RR^2\) to the codomain of \(\RR\).\par
The domains of functions of two variables will be some subset of \(\RR^2\), which can be written as some rule that \(x\) and \(y\) must follow. 
\begin{example}
    Find the domains of the two functions 
    \[ f(x,y) = \frac{\sqrt{x+y+1}}{x-1}\quad g(x,y) = x\ln(y^2-x) \]
    \bf{Solution: }Because the input of a square root must be positive, the domain of \(f\) is all two-tuples \((x,y)\) such that \(x+y+1\geq 0\). We also must have \(x\neq 1\) because of the denominator. We can compactly write this as 
    \[ D = \{(x,y)|x+y+1\geq 0, x\neq 1\}\]
    The domain of \(g\) can be found through a similar process. The function \(\ln\) necessitates a positive input, so our domain is 
    \[ D = \{(x,y)|y^2-x>0\}\]
\end{example}
\begin{example}
    Find the domain and range of \[ g(x, y) = \sqrt{9-x^2-y^2} \]
    \bf{Solution: }The domain is 
    \[ D = \{(x, y)|9-x^2-y^2\geq 0\}\]
    The maximum value attained by \(g\) is \(3\), when \(x=y=0\). The minimum value of \(g\) is \(0\), when \(9-x^2-y^2=0\). Therefore, the range is \([0, 3]\). We can write the range more generally as 
    \[ R = \{z|z=g(x,y), (x,y)\in D\}\]
\end{example}
\subsubsection{Graphs}
Another way of visualizing the behavior of a function of two variables is with a graph. 
\begin{definition}
    If \(f\) is a function of two variables with domain \(D\), then the \bf{graph} of \(f\) is the set of all points \((x,y,z)\) in \(\RR^3\) such that \(z=f(x,y)\) and \((x,y)\in D\).
\end{definition}
We call these graphs in \(\RR^3\) \it{surfaces}.\par
One special type of function are functions of the form
\[ f(x,y) = ax+by+c. \]
These are called \bf{linear functions}. The graphs of these types of functions will be planes in \(\RR^3\).
\subsubsection{Level Curves}
Another way to visualize functions of two variables is by using a \it{level curve}. These maps, also called contour curves, can visualize graphs within a plane. One place where you may have already seen a contour map is with graphs of equipotential lines in physics.
\begin{definition}
    The \bf{level curves} of a function \(f: \RR^2\to \RR\) with range \(R\) are the graphs of curves with equations \(f(x,y)=k\), where \(k\in R\) is a constant. 
\end{definition}
\begin{example}
    Sketch the level curves of the function \(f(x,y)=6-3x-2y\) for the values \(k\in \{-6, 0, 6, 12\}\).\par\bf{Solution: }The curves formed by \(f(x,y)=k\) will be lines in \(\RR^2\),
    \begin{align*}
        6-3x-2y=-6 &\implies y = -\frac{3}{2}x + 6 \\
        6-3x-2y=0 &\implies y = -\frac{3}{2}x + 3 \\
        6-3x-2y=6 &\implies y = -\frac{3}{2}x \\
        6-3x-2y=12 &\implies y = -\frac{3}{2}x-3
    \end{align*}
    Because these lines are equally spaced and parallel, we can reason that \(f\) describes a plane in \(\RR^3\).
\end{example}
\subsubsection{Functions of Three or More Variables}
A \bf{function of three variables}, \(f\), is a rule that assigns a real number to each ordered triple \((x,y,z)\) in a domain \(D\subseteq \RR^3\). For instance, if the temperature depends on the longitude \(x\), latitude \(y\), and time \(t\), we can write \(T=f(x,y,t)\), where \(f: \RR^3\to \RR\).
\begin{example}
    Find the domain of \(f\) if 
    \[ f(x,y,z) = \ln(z-y)+xy\sin z\]
    \bf{Solution: }The input of \(\ln\) must be positive, so the domain is
    \[ D = \{(x,y,z)\in\RR^3|z-y>0\}\]
\end{example}
It is very difficult to visualize functions of three dimensions, since that they would lie in a four-dimensional space. However, we can examine \bf{level surfaces}, which are the surfaces generated by the equations \(f(x,y,z)=k\) for a constant \(k\). 
\begin{example}
    Find the level surfaces for the function \[ f(x,y,z) = x^2+y^2+z^2 \]
    \bf{Solution: }The level surfaces will be of the form \(x^2+y^2+z^2=k\), which we know to describe a sphere centered at \((0,0,0)\) with radius \(\sqrt k\). 
\end{example}
\subsubsection{Functions of Many Variables}
Functions with more than three variables are also possible to consider, although they are exceedingly difficult to visualize. A function of \(n\)-variables is a rule that assigns a number \(z = f(x_1, x_2, \dots, x_n)\) in some domain \(D\subseteq \RR^n\). These (as well as functions of many variables in both the domain \it{and} codomain) will be explored more thoroughly in Linear Algebra.
\subsection{Limits and Continuity}
Let's compare the behavior of the functions 
\[ f(x,y) = \frac{\sin(x^2+y^2)}{x^2+y^2}\quad\text{and}\quad g(x,y)=\frac{x^2-y^2}{x^2+y^2}\]
as \(x\) and \(y\) both approach \(0\).
\picture{\textwidth}{tables.png}
\par Using the above table, we can see that as \((x,y)\to(0,0)\), \(f\) approaches \(1\) from all directions, but \(g\) approaches different values depending on the direction you approach from. Thus, we write
\[ \lim_{(x,y)\to(0,0)}f(x,y) = 1\quad\text{and}\quad \lim_{(x,y)\to(0,0)}g(x,y) \text{ does not exist}\]
In general, the notation
\[ \lim_{(x, y)\to(a, b)}f(x,y)=L \quad\text{or}\quad \lim_{\vec x\to \vec v}f(\vec x) = L\]
indicates that the values of \(f\) approach the number \(L\) as the input approaches \((a,b)\) (or \(\vec v\)) from \it{any} path in the domain of \(f\). A more precise definition follows.
\begin{definition}
    Let \(f: \RR^n\to \RR\) be a function whose domain \(D\) includes points arbitrarily close to \(\vec a\). Then we say that the \bf{limit of \(f\) as \(\vec x\) approaches \(\vec a\) is \(L\)}, and we write 
    \[ \lim_{\vec x\to\vec a}f(\vec x)=L \]
    if for every number \(\epsilon > 0\) there is a corresponding number \(\delta > 0\) such that 
    \[ 0<\norm{\vec x - \vec a}<\delta, \vec x\in D \implies |f(\vec x)-L|<\epsilon \]
\end{definition}
There is a similar definition for vector-valued functions.
\begin{definition}
    Let \(\vec f: \RR^n\to \RR^m\) be a function whose domain \(D\) include points arbitrarily close to \(\vec a\). Then, \[ \lim_{\vec x\to\vec a}\vec f(\vec x) = \vec L\]
    where \(\vec a \in \RR^n\) and \(\vec L \in \RR^m\) if and only if for every \(\epsilon > 0\), there exists some \(\delta > 0\) such that 
    \[ 0<\norm{\vec x - \vec a} < \delta, \vec x\in D\implies \norm{\vec f(\vec x)-\vec L}<\epsilon. \]
\end{definition}
\begin{example}
    Show that \[\lim_{(x, y)\to(0,0)}\frac{x^2-y^2}{x^2+y^2}\] does not exist. \par\bf{Solution: }First, set \(y=0\) and find that \(\lim\limits_{x\to 0}f(x, 0) = \lim\limits_{x\to 0}\frac{x^2}{x^2}=1\). Then, set \(x=0\) and find that \(\lim\limits_{y\to 0}f(0, y) = \lim\limits_{y\to 0}\frac{-y^2}{y^2}=-1\). Because these values are different, \(f\) does not approach the same value from all paths and the limit does not exist.
\end{example}
\begin{example}
    If \(f(x,y)=xy/(x^2+y^2)\), does \(\lim\limits_{(x,y)\to(0,0)}f(x,y)\) exist?\par\bf{Solution: }Analyzing the limit along the \(x\) and \(y\) axes, we can find that along the line \(y=0\),
    \[ \lim_{x\to 0}f(x, 0) = \lim_{x\to 0}\frac{0}{x^2}=0\]
    and
    \[ \lim_{y\to 0}f(0, y) = \lim_{y\to 0}\frac{0}{y^2}=0\]
    However, if  analyze the limit along the line \(y=x\), we can see
    \[ \lim_{x\to 0}f(x, x) = \lim_{x\to 0}\frac{x^2}{2x^2}=\frac{1}{2}\]
    Because we've found two paths that give us a different value, the limit does not exist.
\end{example}
\begin{example}
    If \(f(x,y)=\frac{xy^2}{x^2+y^4}\), does \(\lim\limits_{(x,y)\to(0,0)}f(x,y)\) exist?\par\bf{Solution: }First, we'll look at the limit along all lines \(y=mx\) for \(m\in\RR\). This gives us
    \[ \lim_{x\to 0}f(x, mx) = \lim_{x\to 0}\frac{m^2x^3}{x^2+m^4x^4} = 0\]
    So as \((x,y)\) approaches \((0,0)\) along any \it{linear} path, \(f(x,y)\) approaches \(0\). However, let's look at a nonlinear path, such as \(y=\sqrt x\). Then,
    \[ \lim_{x\to 0}f(x, \sqrt x) = \lim_{x\to 0}\frac{x^2}{x^2+x^2} = \frac{1}{2} \]
    So the limit does not exist.
\end{example}
This process of looking at several possible paths is ultimately going to be fruitless, because we can come up with infinitely many unique ways to approach \((0,0)\). Instead, if we want to look at limits that do exist, we'll have to get more clever.\par
First up, let's look at some limit laws. 
\begin{theorem}[Properties of Multivariable Limits]
    Let \(f: \RR^n: \RR\) and \(g: \RR^n: \RR\) be functions with domains including points arbitrarily close to \(\vec x_0\). Let \(h: \RR\to\RR\) be a function of a single variable that is continuous at \(x=a\), let \(c\in\RR\), and let
    \[ \lim_{\vec x\to \vec x_0}f(\vec x) = a\quad\text{and}\quad\lim_{\vec x\to\vec x_0}g(\vec x)=b\]
    Then the following equalities hold:
    \begin{align*}
        &\lim_{\vec x \to \vec x_0}f(\vec x)g(\vec x) = \lim_{\vec x \to \vec x_0}f(\vec x)\lim_{\vec x \to \vec x_0}g(\vec x) = ab \\
        &\lim_{\vec x \to \vec x_0}f(\vec x) + g(\vec x)=\lim_{\vec x \to \vec x_0}f(\vec x) + \lim_{\vec x \to \vec x_0}g(\vec x) = a+b \\
        &\lim_{\vec x \to \vec x_0}\bqty{f(\vec x)}^c = \bqty{\lim_{\vec x \to \vec x_0}f(\vec x)}^c = a^c \\
        &\lim_{\vec x \to \vec x_0}\bqty{f(\vec x)}^{g(\vec x)} = \bqty{\lim_{\vec x \to \vec x_0}f(\vec x)}^{\lim\limits_{\vec x \to \vec x_0}g(\vec x)} = a^b \\
        &\lim_{\vec x \to \vec x_0}cf(\vec x) = c\lim_{\vec x \to \vec x_0}f(\vec x) = ca \\
        &\lim_{\vec x \to \vec x_0}\bqty{f(\vec x) + c} = \bqty{\lim_{\vec x \to \vec x_0}f(\vec x)}+c = a+c \\
        &\lim_{\vec x\to \vec x_0}h(f(\vec x)) = h\bqty{\lim_{\vec x\to \vec x_0}f(\vec x)} = h(a)
    \end{align*}
\end{theorem}
\begin{example}
    Find \(\lim\limits_{(x, y)\to(0,0)}\frac{3x^2y}{x^2+y^2}\) if it exists.\par\bf{Solution: }based on how the function looks, it seems that the numerator will shrink faster than the denominator and the limit will be \(0\). To prove this, we can use the epsilon-delta definition.\par Suppose \(\epsilon > 0\). Then, we need to find some \(\delta > 0\) such that 
    \[ \abs{\frac{3x^2y}{x^2+y^2}-0}<\epsilon\quad\text{whenever}\quad 0<\sqrt{x^2+y^2}<\delta \]
    We can rewrite this to say
    \[ \frac{3x^2|y|}{x^2+y^2}<\epsilon\quad\text{whenever}\quad 0<\sqrt{x^2+y^2}<\delta \]
    Because \(x^2\leq x^2+y^2\) (as \(y^2\) is always positive), we can say 
    \[ \frac{3x^2|y|}{x^2+y^2}\leq \frac{3x^2|y|}{x^2} = 3|y| = 3\sqrt{y^2}\]
    Because \(\sqrt{y^2} \leq \sqrt{x^2+y^2}\), we can say
    \[ 3\sqrt{y^2} \leq 3\sqrt{x^2+y^2} \]
    If we choose \(\delta = \epsilon/3\), then
    \[ \abs{f(x, y)-0}\leq 3\sqrt{x^2+y^2}\leq 3\delta = \epsilon\]
    So the proof is complete and \(\lim\limits_{(x,y)\to(0,0)}f(x,y)=0\).
\end{example}
\subsubsection{Continuity}
Recall that evaluating limits of \it{continuous} functions of a single variable is easy, using direct substitution. This same property applies to continuous functions of multiple variables.
\begin{definition}
    A function \(\vec f: \RR^n\to \RR^m\) is \bf{continuous} at \(\vec x_0\in\RR^n\) if and only if 
    \[ \lim_{\vec x\to \vec x_0}\vec f(\vec x) = \vec f(\vec x_0)\]
    If \(f\) is continuous on every point in a subset \(S\subseteq \RR^n\), we say that \(f\) is continuous on \(S\).
\end{definition}
Polynomial functions of several variables are functions of the form
\[ f(x_1, x_2, \dots, x_n) = \sum_{i} cx_1^{m_{i,1}}x_2^{m_{i,2}}\cdots x_n^{m_{i,n}}\]
Where all exponents are non-negative integers. \par
Rational functions of several variables are functions of the form \(f(x)=g(x)/h(x)\) where \(g\) and \(h\) are polynomial functions. \par
Because polynomial functions are only composed of addition and multiplication, limit properties can be used to find that all polynomials with the domain are continuous on their entire domain. From this property, we can also determine that all rational functions are continuous on their entire domain.
\begin{example}
    Evaluate 
    \[ \lim_{(x,y)\to(1,2)}\bqty{x^2y^3-x^3y^2+3x+2y}\]
    \bf{Solution: }We can just plug in, as this is a continuous polynomial function.
    \[ \lim_{(x,y)\to(1,2)}\bqty{x^2y^3-x^3y^2+3x+2y} = 1^2\cdot 2^3 - 1^3\cdot 2^2 + 3\cdot 1 + 2\cdot 2 = 11 \]
\end{example}
\begin{example}
    Let \[ f(x) = \begin{cases}
        \frac{x^2-y^2}{x^2+y^2} & (x,y)\neq (0, 0) \\
        0 & (x,y)=(0,0)
    \end{cases}\quad\text{and}\quad g(x)=\begin{cases}
        \frac{3x^2y}{x^2+y^2} & (x,y)\neq(0,0) \\
        0 & (x, y) = (0,0)
    \end{cases}\]
    Is \(f\) continuous at \((0,0)\)? Is \(g\)?\par\bf{Solution: }First, we can remember from the previous section that \(\lim\limits_{(x,y)\to(0,0)}f(x,y)\) does not exist. Therefore, \(f\) is not continuous at \((0,0)\), even with the hole patched. \par
    For \(g\), recall that \(\lim\limits_{(x,y)\to(0,0)}g(x,y)=0\). Therefore, \(g(x,y)\) is defined and has an existing limit at \((0,0)\), and those values are the same. Therefore, \(g\) is continuous.
\end{example}
\begin{example}
    Where is the function \(h(x,y)=\arctan(y/x)\) continuous?\par\bf{Solution: }The function \(f(x,y)=y/x\) is continuous for all \((x,y)\in\RR^2, x\neq 0\). \(\arctan u\) is continuous everywhere. Therefore, the composite function \(\arctan(f(x, y))\) is continuous everywhere except where \(x=0\).
\end{example}
\subsubsection{Partial Derivatives}
Partial derivatives give us a way to analyze the rates of change of functions when only one of their parameters is allowed to vary. \par
If \(f(x,y)\) is a function of two variables, its \it{partial derivatives} are defined as
\[ \pdv{f}{x} = \lim_{h\to 0}\frac{f(x+h, y)-f(x,y)}{h}\quad\text{and}\quad\pdv{f}{y}=\lim_{x\to 0}\frac{f(x, y+h)-f(x,y)}{h}\]
These are also sometimes written as \(f_x\) and \(f_y\) or \(\partial_xf\) and \(\partial_yf\). The values of these partial derivatives correspond to the rate of change of \(f\) when only one parameter (the one being differentiated with) is allowed to vary, and the other is held fixed.\par
To find partial derivatives, simply find the derivative as normal, but treat all variables besides the one being differentiated with as constants.
\begin{example}
    Find \(f_x\) and \(f_y\) if \(f(x,y)=x^3+x^2y^3-2y^2\).\par\bf{Solution: }Differentiate.
    \begin{align*}
        \pdv{f}{x} &= 3x^2+2xy^3 \\
        \pdv{f}{y} &= 3x^2y^2-4y
    \end{align*}
\end{example}
\subsubsection{Interpretations of Partial Derivatives}
Let the equation \(z=f(x,y)\) represent a surface \(S\) in \(\RR^3\). If \(f(a,b)=c\), then the point \(P(a,b,c)\) lies on \(S\). By fixing \(y=b\) and allowing \(x\) to vary, we are analyzing the change in the \(z\)-coordinate as \(x\) moves away from \(x=a\). In other words, we are restricting our view only to the intersection of the plane \(y=b\) with \(S\).\par
Further, the partial derivatives \(f_x(a,b)\) and \(f_y(a,b)\) can be interpreted geometrically as the slopes of the tangent lines to the traces of \(S\) formed by the intersection of \(S\) with the planes \(y=b\) and \(x=a\).
\begin{example}
    Find \(\partial_xz\) and \(\partial_yz\) if \(z\) is defined implicitly under the equation 
    \[ x^3+y^3+z^3+6xyz=1\]
    \bf{Solution: }First, implicitly (partial) differentiate with respect to \(x\),
    \[ 3x^2+3z^2\pdv{z}{x}+6yz+6xy\pdv{z}{x} = 0\]
    and solve for \(\partial_xz\),
    \[ \pdv{z}{x} = \frac{-x^2-2yz}{z^2+2xy}\]
    Then do the same for \(\partial_yz\):
    \[ 3y^2+3z^2\pdv{z}{y}+6xz+6xy\pdv{z}{y} = 0\implies \pdv{z}{y} = \frac{-y^2-2xz}{z^2+2xy}\]
\end{example}  
\subsubsection{Functions of more than two variables}
Partial derivatives are defined in much the same way for functions of multiple variables.
\begin{definition}
    If \(f(x_1, x_2, \dots, x_n)\) is differentiable, then \(\partial_{x_i}f\) is defined as
    \[ \pdv{f}{x_i} = \lim_{h\to 0}\frac{f(x_1, \dots, x_i + h, \dots, x_n) - f(x_1, \dots, x_i, \dots, x_n)}{h}\]
\end{definition}
\subsubsection{Higher Derivatives}
If \(f(x,y)\) is a function of two variables, then its partial derivatives \(f_x\) and \(f_y\) are also functions of two variables, and can be partially differentiated as normal. Therefore, we must consider the \bf{second partial derivatives} of \(f\): \((f_x)_x, (f_y)_x, (f_x)_y\) and \((f_y)_y\). The following notation is used:
\[ (f_a)_b = f_{ab} = \frac{\partial^2f}{\partial a \partial b}\]
\begin{example}
    Find all second partial derivatives of 
    \[ f(x,y) = x^3+x^2y^3-2y^2 \]
    \bf{Solution: }First, find the first order partials,
    \[ f_x = 3x^2+2xy^3\quad\text{and}\quad f_y=3x^2y^2-4y \]
    Then, find the second order partials
    \[ f_{xx} = 6x+2y^3\quad\quad f_{yy} = 6x^2y - 4\]
    \[ f_{xy} = 6xy^2\quad\quad f_{yx} = 6xy^2 \]
\end{example}
Note that \(f_{xy}=f_{yx}\). This is not a coincidence.
\begin{theorem}[Clairaut's Theorem]
    Suppose \(f\) is defined on a disk \(D\) that contains the point \((a,b)\). If the functions \(f_{xy}\) and \(f_{yx}\) are both continuous on \(D\), then
    \[ f_{xy}(a,b) = f_{yx}(a,b)\]
\end{theorem}
This theorem will extend to functions of more than two variables and partial derivatives of higher order as well.
\subsubsection{Partial Differential Equations}
Any equation that contains a partial derivative is called a partial differential equation. For instance, the equation 
\[ \pdv[2]{u}{x} + \pdv[2]{u}{y}=0\]
is called \bf{Laplace's equation}. Solutions to this equation are called \bf{harmonic functions} and are important in many applications, especially in thermodynamics, fluids, and electricity.\par The methods for solving these types of equations are outside the scope of this course. You'll learn more about it in the aptly named class ``partial differential equations.''
\subsubsection{The Cobb-Douglas Production Problem}
We're going to take a break from the pure math and look at an application for a bit.\par
Consider a production function \(P(K, L)\) that describes how much of a good a society will produce. The parameters of this function are capital and labor for \(K\) and \(L\) respectively. The partial derivative \(\partial_LP\) is the rate at which production changes with a change in labor. Economists refer to this as the \it{marginal productivity of labor}. Likewise, the partial derivative \(\partial_KP\) is the \it{marginal productivity of capital}. The assumptions made by Cobb and Douglas to describe production are as follows:
\begin{enumerate}
    \item If either capital or labor vanishes, then so will production
    \item The marginal productivity of labor is proportional to the amount of production per unit of labor 
    \item The marginal productivity of capital is proportional to the amount of production per unit of capital.
\end{enumerate}
Because the production per unit of labor is \(P/L\), assumption 2 says
\[ \pdv{P}{L} = \alpha \frac{P}{L}\]
If we take capital to be constant at \(K=K_0\), then this partial differential equation becomes a total differential equation
\[ \dv{P}{L} = \alpha\frac{P}{L} \]
Solving this separable differential equation, we get
\begin{align*}
    \frac{\dd P}{P} &= \alpha \frac{\dd L}{L} \\
    \ln P &= \alpha \ln L + C(K_0) \\
    P &=  C_1(K_0)L^\alpha
\end{align*}
\(C_1\) is a function of \(K_0\) because the value of \(C\) will vary depending on what \(K_0\) is.\par Doing the same process, except taking labor to be constant at \(L=L_0\), we find that 
\[ P = C_2(L_0)L^\beta \]
Combining these two equations, we get
\[ P = bL^\alpha K^\beta \]
Where \(b\) is a positive constant independent of both \(L_0\) and \(K_0\). From assumption 1, we can guarantee that \(\alpha, \beta  > 0\). Further, if we increase both labor and capital by a constant factor \(m\), then
\begin{align*}
    P' &= b(L+m)^\alpha (K+m)^\beta \\
    &= bm^{\alpha + \beta}L^\alpha K^\beta = m^{\alpha + \beta}P
\end{align*}
If \(\alpha + \beta = 1\), the \(P'=mP\). Because of this, Cobb and Douglas assumed \(\alpha + \beta = 1\) and thus 
\[ P = bL^\alpha K^{1-\alpha}\]
This is the famous \it{Cobb-Douglas production function} that is widely used in the study of economics.
\subsection{Tangent Planes and Linear Approximations}
\subsubsection{Tangent Planes}
One of the most important results of single-variable calculus is the idea of using the tangent line to approximate the values of functions. In this section we will develop a similar idea for functions of multiple variables by using planes to approximate values.\par
Suppose a surface \(S\) has equation \(z=f(x,y)\) where \(f\) has continuous partial derivative, and let \(P_0(x_0, y_0, z_0)\) be a point on \(S\). Then, let \(C_1\) and \(C_2\) be the curves formed by the intersection between \(S\) and the planes \(y=y_0\) and \(x=x_0\) respectively. Let \(T_1\) and \(T_2\) be tangent to these curves at \(P\). Then the \bf{tangent plane} to \(S\) at \(P\) is the plane containing both tangent lines \(T_1\) and \(T_2\).\par
In fact, all possible tangents to \(S\) will lie in this plane (we will show why this is true later). This tangent plane can be used to approximate values of \(f\) near \(P\).\par
We know that the equation of a plane will be of the form
\[ A(x-x_0) + B(y-y_0) + C(z - z_0)=0.\]
If we divide by \(C\) and define \(a=-A/C\), \(b=-B/C\), we get
\[ a(x-x_0)+b(y-y_0)=z-z_0\]
setting \(y=y_0\), we get the two lines
\begin{align*}
    y &= y_0 \\
    z-z_0 &= a(x-x_0)
\end{align*}
We can recognize this as the point-slope equation for a line, with slope \(a\). However, we know that the \(T_1\) must have slope \(f_x(x_0, y_0)\). Therefore,
\[ f_x(x_0, y_0) = a\]
We can repeat this process, putting \(x=x_0\) into the equation for the plane to find 
\[ f_y(x_0, y_0) = b\]
Therefore, the equation for the tangent plane is
\[ f_x(x_0,y_0)\bqty{x-x_0} + f_y(x_0,y_0)\bqty{y-y_0}=z-z_0\]
\begin{example}
    Find the tangent plane to the elliptic paraboloid \(z=2x^2+y^2\) at the point \((1,1,3)\).\par\bf{Solution: }First, compute \(z_x(1,1)\) and \(z_y(1,1)\).
    \begin{align*}
    z_x &= 4x \implies z_x(1,1) = 4\\
    z_y &= 2y \implies z_y(1,1) = 2
    \end{align*}
    So the equation for the tangent plane is
    \[ z-3 = 4(x - 1) + 2(y - 1)\]
\end{example}
\subsubsection{Linear Approximations}  
Just as we used tangent lines to approximate values in single-variable calculus, we can use tangent planes to approximate values in multivariable functions. If \(L(x,y)\) gives the \(z\) coordinate of the tangent plane to \(f(x,y)\) at \((x_0, y_0)\) in terms of the \(x\) and \(y\) coordinates, then \(L\)--called the \it{linearization} of \(f\) at \((x_0, y_0)\)--is a good approximation of \(f\) near \((x_0, y_0)\). \par
In the previous section, we found a tangent plane to \(f(x,y)=2x^2+y^2\) at \((1,1)\) to be given by the equation
\[ z - 3 = 4(x-1) + 2(y-1) \]
Rearranging this, we get
\[ L(x, y) = 4x+2y-3 \]
Using this function, we can approximate values near \((1,1,3)\). For instance,
\[ f(1.1, 0.95) \approx L(1.1, 0.95) = 4.4 + 1.9 - 3 = 3.3 \]
Compared to the true value of \(f(1.1, 0.95)\) of \(3.3225\), this is a pretty good approximation.\par
We have defined tangent planes for surfaces with continuous first partial derivatives, but what if this isn't the case? For instance, consider the surface described by the equation
\[ f(x,y) = \begin{cases}
    \frac{xy}{x^2+y^2}&(x,y)\neq(0,0) \\
    0&(x,y)=(0,0)
\end{cases}\]
Computing the first partial derivatives, we get that \(f_x(0,0)=f_y(0,0)=0\). However, they are not continuous, because \(\lim\limits_{(x,y)\to (0,0)}f_x(x,y)\) does not exist (and likewise for \(f_y\)). The tangent plane approximation of \(f\) at \((0,0)\) would be \(f(x,y)\approx 0\), but \(f(x,y)=\frac{1}{2}\) for all points on the line \(y=x\), so this tangent plane approximation is clearly inaccurate. To avoid cases like this, we will formula the idea of differentiability for multivariable functions. \par 
\begin{definition}
    A function \(f(x,y)\) is \bf{differentiable} at a point \((a, b)\) if and only if, for all points \((x,y)\) in a \(\delta\) disk around \((a,b)\), \(f\) can be expressed as
    \[ f(x, y) = f(a, b) + f_x(a,b)[x-a] + f_y(a,b)[y-a] + E(x,y)\]
    where the error term \(E\) satisfies 
    \[ \lim_{(x,y)\to(a,b)}\frac{E(x,y)}{\sqrt{([x-a]^2+[y-b]^2)}}=0\]
\end{definition}
In English, this is essentially saying that if, for any point near \((a,b)\), we can express \(f(x,y)\) as the sum of the value of its tangent plane at \((a,b)\) and some error term that becomes arbitrarily small as \((x,y)\to(a,b)\). In other words, \(f\) ``looks like'' its tangent plane near \((a,b)\).\par
While the formal definition of differentiability is good to know, it is a very cumbersome and difficult definition to work with. Luckily, there is a much more simple way to check for differentiability.
\begin{theorem}
    If \(f: \RR^n\to \RR\) is a multivariable function, it is differentiable at a point \((a,b)\) if all of its first partial derivatives exist near \((a,b)\) and are continuous at \((a,b)\).
\end{theorem}
\begin{example}
    Show that \(f(x,y)=xe^{xy}\) is differentiable at \((1, 0)\) and find its linearization there. Then, use that to approximate \(f(1.1, -0.1)\).
    \par\bf{Solution: }The partial derivatives are:
    \[ f_x(x,y) = e^{xy}+xye^{xy}\quad f_x(1, 0) = 1\]
    \[ f_y(x,y) = x^2e^{xy} \quad f_y(1,0) = 1\]
    Because both partial derivatives are continuous at \((1, 0)\), \(f\) is continuous there as well. Further, the linearization is given by
    \[ L(x,y) = f(1, 0) + f_x(1,0)(x-1) + f_y(1,0)y \]
    or
   \[ L(x,y) = 1 + x - 1 + y = x + y\]
   With this approximation, we find \( f(1.1, -0.1)\approx L(1.1, -0.1) = 1 \). Compared to the actual value \(f(1.1, -0.1) \approx =0.98542\), this is a pretty good approximation.
\end{example}
\subsubsection{Differentials}
For functions of single variables, we defined the differential \(\dd x\) to be an independent variable. Then, \(\dd y\) can be defined in terms of \(\dd x\):
\[ \dd y = f'(x)\dd x\]
Now, consider a function of multiple variables \(z = f(x_1, x_2, \dots, x_n)\). We can define \(\dd x_1\), \(\dd x_2\), and so on until \(\dd x_n\) to be independent variables, and then write
\[ \dd z = \pdv{f}{x_1}\dd x_1 + \pdv{f}{x_2}\dd x_2 + \dots + \pdv{f}{x_n}\dd x_n\]
Further, we can say that
\[ f(x_1 + \dd x_1, x_2 + \dd x_2, \dots, x_n + \dd x_n) \approx f(x_1, x_2, \dots, x_n) + \dd z\]
As the size of the differentials increases, this approximation gets less and less accurate, since it only uses the tangent plane instead of the full surface.
\begin{example}
    If \(z=f(x,y)=x^2+3xy-y^2\), find an expression for \(\dd z\) in terms of \(x\), \(y\), \(\dd x\), and \(\dd y\). Then, with \(x = 2\), \(y=3\), \(\dd x = 0.05\) and \(\dd y = -0.04\), compare the values of \(\dd z\) and \(f(x + \dd x, y + \dd y) - f(x,y)\).
    \par\bf{Solution: }First, finding the differential,
    \begin{align*}
        \dd z &= f_x(x,y)\dd x + f_y(x, y)\dd y \\
        &= \bqty{2x+3y}\dd x + \bqty{3x-2y}\dd y
    \end{align*}
    and plugging in the values,
    \[ \dd z = \bqty{2(2) + 3(3)} 0.05+\bqty{3(2) - 2(3)}(-0.04) = 0.65 \]
    Comparing this to \(f(x + \dd x, y + \dd y) - f(x,y) \),
    \[ \Delta z = f(2.05, 2.96) - f(2, 3) = 0.6449\]
    So \(\dd z\) is a pretty good approximation for \(\Delta z\).
\end{example}
\begin{example}
    The base radius of a right circular cone are measured as \(10 \unit{\centi\metre}\) and \(25 \unit{\centi\metre}\) respectively, with a possible error in each measurement of at most \(0.1\unit{\centi\metre}\). Use differentials to approximate the maximum error in the calculated volume cone.
    \par\bf{Solution: }The volume \(V\) of a cone with radius \(r\) and height \(h\) is \(V = \pi r^2h/3\). Then, the differential \(\dd V\) is 
    \[ \dd V = \bqty{\frac{\pi r^2}{3}}\dd h + \bqty{\frac{2\pi rh}{3}}\dd r\]
    Since the error is at most \(0.1\unit{\centi\metre}\), \(\abs{\dd h} \leq 0.1\) and \(\abs{\dd r}\leq 0.1\). Taking the maximum error,
    \[ \dd V \leq \bqty{\frac{\pi\pqty{ 10^2}}{3}}0.1+\bqty{\frac{2\pi(10)(25)}{3}}0.1 = 20\pi \]
    So the maximum possible error in the volume is \(20\pi \unit{\centi\metre^3}\approx 63\unit{\centi\metre^3}\)
\end{example}
\subsection{The Multivariable Chain Rule}
Consider a single variable function \(y=f(x(t))\). Then, the chain rule from calculus 1 states
\[ \dv{y}{t} = \dv{y}{x}\dv{x}{t} \]
Now, in the case of multivariable functions, we can use the \it{multivariable chain rule}.
\begin{theorem}
    If \(z=f(x_1, x_2, \dots, x_n)\) is a differentiable function, and \(x_1=x(t)\), \(x_2=x_2(t)\), and so on are all differentiable single variable functions, the derivative of \(z\) with respect to \(t\) is 
    \[ \dv{z}{t} = \sum_{i=1}^n\pdv{f}{x_i}\dv{x_i}{t} \]
\end{theorem}
\begin{proof}
    Let \(z=f(\vec x)\) be a multivariable function where \(\vec x = \< x_1, x_2, \dots, x_n \>\) that is differentiable at a point \(\vec u = \< u_0, u_1, \dots, u_n \>\). Let \(x_1=x_1(t)\), \(x_2=x_2(t)\), and so on be differentiable functions with time. Then, a change in time \( \Delta t\) causes a change \(\Delta x_1\) in \(x_1\), \(\Delta x_2\) in \(x_2\), and so on. These, in turn, cause a change \(\Delta z\) in \(z\). We can write
    \[ \Delta z = \pdv{f}{x_1}\Delta x_1 + \pdv{f}{x_2}\Delta x_2 + \dots + \pdv{f}{x_n}\Delta x_n  + E(\vec x) \]
    where the error term \(E\) satisfies
    \[ \lim_{\vec x\to \vec u}E(\vec x) = 0\]
    If we divide both sides by \(\Delta t\), we get
    \[ \frac{\Delta z}{\Delta t} = \pdv{f}{x_1}\frac{\Delta x_1}{\Delta t} + \pdv{f}{x_2}\frac{\Delta x_2}{\Delta t} + \dots + \pdv{f}{x_n}\frac{\Delta x_n}{\Delta t} + \frac{E(\vec x)}{\Delta t} \]
    Now, if we let \(\Delta t\to 0\), 
    \begin{align*} \lim_{\Delta t\to 0}\frac{\Delta z}{\Delta t} &= \lim_{\Delta t\to 0}\bqty{\pdv{f}{x_1}\frac{\Delta x_1}{\Delta t} + \pdv{f}{x_2}\frac{\Delta x_2}{\Delta t} + \dots + \pdv{f}{x_n}\frac{\Delta x_n}{\Delta t} + \frac{E(\vec x)}{\Delta t}} \\
    &= \pdv{f}{x_1}\lim_{\Delta t\to 0}\frac{\Delta x_1}{\Delta t} + \pdv{f}{x_2}\lim_{\Delta t\to 0}\frac{\Delta x_2}{\Delta t} + \dots + \pdv{f}{x_n}\lim_{\Delta t \to 0}\frac{\Delta x_n}{\Delta t} + \lim_{\Delta t\to 0}\frac{E(\vec x)}{\Delta t}
    \end{align*}
    Now, because \(\lim\limits_{\Delta t\to 0}\vec x = \vec u\), we can rewrite as
    \begin{align*}
        \lim_{\Delta t\to 0}\frac{\Delta z}{\Delta t} &= \pdv{f}{x_1}\lim_{\Delta t\to 0}\frac{\Delta x_1}{\Delta t} + \pdv{f}{x_2}\lim_{\Delta t\to 0}\frac{\Delta x_2}{\Delta t} + \dots + \pdv{f}{x_n}\lim_{\Delta t \to 0}\frac{\Delta x_n}{\Delta t} + \cancelto{0}{\frac{\lim_{\vec x\to\vec u}E(\vec x)}{\lim_{\Delta t\to 0}\Delta t}} \\
        &= \pdv{f}{x_1}\lim_{\Delta t\to 0}\frac{\Delta x_1}{\Delta t} + \pdv{f}{x_2}\lim_{\Delta t\to 0}\frac{\Delta x_2}{\Delta t} + \dots + \pdv{f}{x_n}\lim_{\Delta t \to 0}\frac{\Delta x_n}{\Delta t}
    \end{align*}
    Taking note that \(\lim\limits_{\Delta t \to 0}\frac{\Delta z}{\Delta t} = \dv{z}{t}\) and \(\lim\limits_{\Delta t\to 0}\frac{\Delta x_i}{\Delta t} = \dv{x_i}{t}\), rewrite:
    \[ \dv{z}{t} = \pdv{f}{x_1}\dv{x_1}{t} + \pdv{f}{x_2}\dv{x_2}{t}+\dots+\pdv{f}{x_n}\dv{x_n}{t} = \sum_{i=1}^n\pdv{f}{x_i}\dv{x_i}{t} \]
    And we've reached the desired result.    
\end{proof}
\begin{example}
    If \(z=x^2y+3xy^4\), where \(x=\sin 2t\) and \(y=\cos t\), find \(\partial z/\partial t\). \par\bf{Solution: }This is as simple as plugging in:
    \begin{align*}
        \dv{z}{t} &= \pdv{z}{x}\dv{x}{t} + \pdv{z}{y}\dv{y}{t} \\
        &= 2\bqty{2xy + 3y^4}\cos 2t - \bqty{x^2+12xy^3}\sin t
    \end{align*}
    We can also get an expression depending only on \(t\) if we plug back in for \(x\) and \(y\),
    \[ \dv{z}{t} = 2\bqty{2(\sin 2t)(\cos t) + 3(\cos t)^4}\cos 2t - \bqty{(\sin 2t)^2 + 12(\sin 2t)(\cos t)^3}\sin t\]
    Note that we could have also plugged in the \(t\)-valued expressions for \(x\) and \(y\) \it{before} differentiating. We would get the same result, but it would be much more difficult and tedious to compute.\par
    We can interpret this result as the rate of change of \(z\) as the point \((x,y)\) moves across the curve described by parametric equations \(x=\sin 2t\) and \(y=\cos t\).
\end{example}
\begin{example}
    The pressure \(P\) (in pascals), volume \(V\), and temperature \(T\) of an ideal gas are related by the equation \(PV=nRT\) where \(n\) is the amount of substance (in moles) and \(R\) is the ideal gas constant, with approximate value \( 8.3144\) \(\unit{\metre^3\cdot \pascal\cdot \kelvin^{-1}\cdot \mol^{-1}}\). \par
    If \( 1000 \) \(\unit{\mol}\) of a gas is in a \(100\) \(\unit{\liter} \) container that is expanding at a rate of \(0.2\) \(\unit{\liter \second^{-1}}\), at a temperature of \(300\) \(\unit{\kelvin}\) that is increasing at a rate of \(0.1\) \(\unit{\kelvin\second^{-1}}\), find the rate of change of the pressure of the gas. \par\bf{Solution: }First, isolate the pressure,
    \begin{align*}
    P &= nRTV^{-1}
    \end{align*}
    Then, apply the multivariable chain rule:
    \begin{align*}
    \dv{P}{t} &= \pdv{P}{T}\dv{T}{t}+\pdv{P}{V}\dv{V}{t} \\
    &= \bqty{nRV^{-1}}\dv{T}{t} +\bqty{-nrTV^{-2}}\dv{V}{t} \\
    &\approx -41.572\enskip\unit{\pascal\cdot \second^{-1}}
    \end{align*}
\end{example}
Now, consider the case where the parameters of a multivariable function are also multivariable functions. This is another special case of the multivariable chain rule.
\begin{theorem}
Let \(z=f(x_1, x_2, \dots, x_n)\), with each parameter \(x_i = g_i(t_1, t_2, \dots, t_m)\) being a multivariable function. Then,
\[ \pdv{z}{t_a} = \sum_{i=1}^n \pdv{z}{x_i}\pdv{x_i}{t_a} \] 
For each \(a = 1, 2, \dots, m\).
\end{theorem}
The proof of this theorem is pretty easy but i dont feel like writing it out so im just gonna kind of explain it \par Find the same expression 
\[ \Delta z = \pdv{f}{x_1}\Delta x_1 + \pdv{f}{x_2}\Delta x_2 + \dots + \pdv{f}{x_n}\Delta x_n  + E(\vec x) \]
and then recognize that each \(\Delta x_i\) can be shown to be equal to
\[ \Delta x_i = \pdv{x_i}{t_a}\Delta t_a + E_i(x_i)\]
Then plug these back into the original expression for \(\Delta z\) and take the limit as \(\Delta t_a\to 0\). \par
The same ideas can be extended to multivariable functions with several layers of parameters.
\begin{example}
If \( u=x^4y+y^2z^3 \), where \(x=rse^t\), \(y=rs^2e^{-t}\), and \(z=r^2s\sin t\), find an expression for \(\partial u/\partial s\).\par\bf{Solution: }By the chain rule,
\begin{align*}
    \pdv{u}{s} &= \pdv{u}{x}\pdv{x}{s} + \pdv{u}{y}\pdv{y}{s}+\pdv{u}{z}\pdv{z}{s} \\
    &= \bqty{4x^3y}re^t + \bqty{x^4+2yz^3}2rse^{-t} + \bqty{3y^2z^2}r^2\sin t
\end{align*}
\end{example}
\begin{example}
If \(g(s,t) = f(s^2-t^2, t^2-s^2)\) and \(f\) is differentiable, show that \(g\) satisfies the equation
\[ t\pdv{g}{s} + s\pdv{g}{t} = 0.\]
\bf{Solution: }First, define \(u=s^2-t^2\) and \(v=t^2-s^2\). Then, \(g(s,t) = f(u, v)\). From there, we can compute each partial derivative:
\begin{align*}
    \pdv{g}{s} &= \pdv{f}{u}\pdv{u}{s} + \pdv{f}{v}\pdv{v}{s} \\
    &= 2s\pdv{f}{u} - 2s\pdv{f}{v} \\
    \pdv{g}{t} &= \pdv{f}{u}\pdv{u}{t} + \pdv{f}{v}\pdv{v}{t} \\
    &= -2t\pdv{f}{u} + 2t\pdv{f}{v} 
\end{align*}  
So the overall expression can be written as
\[ t\pdv{g}{s} + s\pdv{g}{t} = 2st\pdv{f}{u} - 2st\pdv{f}{v} - 2st\pdv{f}{u} + 2st\pdv{f}{v} = 0 \]
\end{example}
\begin{example}
If \(z=f(x,y)\) has continuous second-order partial derivatives, \(x=r^2+s^2\) and \(y=2rs\), find \(\partial z/\partial r\) and \(\partial^2z/\partial r^2\).\par\bf{Solution: }The chain rule tells us
\[ \pdv{z}{r} = \pdv{z}{x}\pdv{x}{r} + \pdv{z}{y}\pdv{y}{r} = \pdv{z}{x}2r + \pdv{z}{y}2s \]
Then, differentiating again,
\begin{align*}
    \pdv[2]{z}{r} &= \bqty{2\pdv{z}{x}+2r\pdv{r}\pqty{\pdv{z}{x}}} + \bqty{2s\pdv{r}\pqty{\pdv{z}{y}}}
\end{align*} 
Applying the chain rule to the inner terms, we find that
\begin{align*}
    \pdv{r}\pqty{\pdv{z}{x}} &= \pdv{x}\pqty{\pdv{z}{x}}\pdv{x}{r} + \pdv{y}\pqty{\pdv{z}{x}}\pdv{y}{r} \\
    &= 2r\pdv[2]{z}{x} + 2s\frac{\partial^2z}{\partial x\partial y} \\
    \pdv{r}\pqty{\pdv{z}{y}} &= \pdv{x}\pqty{\pdv{z}{y}}\pdv{x}{r} + \pdv{y}\pqty{\pdv{z}{y}}\pdv{y}{r} \\
    &= 2r\frac{\partial^2z}{\partial x\partial y} + 2s\pdv[2]{z}{y}
\end{align*}
Plugging these back into the expression for \(\partial^2z/\partial r^2\),
\begin{align*}
    \pdv[2]{z}{r} &= 2\pdv{z}{x} + 2r\bqty{2r\pdv[2]{z}{x}+2s\frac{\partial^2z}{\partial x\partial y}}+2s\bqty{2r\frac{\partial^2z}{\partial x\partial y}+2s\pdv[2]{z}{y}} \\
    &= 2\pdv{z}{x} + 4r^2\pdv[2]{z}{x} + 4s^2\pdv[2]{z}{y} + 8rs\frac{\partial^2z}{\partial x\partial y}
\end{align*}
\end{example}
\subsubsection{Implicit Differentiation}
Suppose that an equation of the form \(F(x,y)=0\) defines \(y\) implicitly as a differentiable function of \(x\). That is, \(y=f(x)\), where \(F(x, f(x))=0\) for all \(x\) in the domain of \(f\). If \(F\) is differentiable, we can apply the multivariable chain rue to obtain
\[ \pdv{F}{x}\dv{x}{x} + \pdv{F}{y}\dv{y}{x} = 0 \]
Because \(\dd x/\dd x\) is just \(1\), this simplifies to
\[ \pdv{F}{x} + \pdv{F}{y}\dv{y}{x} = 0\]
Rearranging some terms, we obtain
\[ \dv{y}{x} = -\frac{\partial F/\partial x}{\partial F/\partial y} \]
To derive this result, we assumed that \(F(x,y)\) implicitly defines a function \(y\) of \(x\). The \bf{Implicit Function Theorem}, proved in advanced calculus, tells us when this assumption is valid. It states that if \(F\) is defined on a disk containing \((a,b)\), where \(F(a,b)=0\), \(F_y(a,b)\neq 0\), and \(F_x\) and \(F_y\) are continuous on the disk, then the equation \(F(x,y)\) defines \(y\) as a function of \(x\) near \((a, b)\) and the derivative of the function is given by the above formula. 
\begin{example}
Find \(\dd y/\dd x\) if \(x^3+y^3=6xy\).\par\bf{Solution: }First, move all terms over to one side, so \(x^3+y^3-6xy=0\). Then, we can say that
\[ \dv{y}{x} = -\frac{3x^2-6y}{3y^2-6x} = -\frac{x^2-2y}{y^2-2x}\]
\end{example}
A similar rule also exists for an implicit definition of \(z\) as \(F(x,y,z)=0\). Partially differentiating both sides, we get one of the two following formulas,
\begin{align*}
\pdv{F}{x}\pdv{x}{x}+\pdv{F}{y}\pdv{y}{x}+\pdv{F}{z}\pdv{z}{x} &= 0 \\
\pdv{F}{x}\pdv{x}{y}+\pdv{F}{y}\pdv{y}{y}+\pdv{F}{z}\pdv{z}{y} &= 0
\end{align*}
Because \(\partial y/\partial x = \partial x/\partial y = 0\) and \(\partial /\partial x =\partial y/\partial y = 0\), these formulas become
\begin{align*}
\pdv{F}{x}+\pdv{F}{z}\pdv{z}{x} &= 0 \\
\pdv{F}{y}+\pdv{F}{z}\pdv{z}{y} &= 0
\end{align*}
Which can be rearranged to give us 
\[ \pdv{z}{x} = -\frac{\partial F/\partial x}{\partial F/\partial z}\quad\text{and}\quad \pdv{z}{y} = -\frac{\partial F/\partial y}{\partial F/\partial z}\]
Again, we can use the implicit function theorem to say when the assumption that \(F(x,y,z)\) implicitly defined \(z\) in terms of \(x\) and \(y\) is valid. If \(F\) is defined within a sphere containing \((a,b,c)\), where \(F(a,b,c)=0\), \(F_z(a,b,c)\neq 0\), and \(F_x\), \(F_y\), and \(F_z\) are all continuous inside the sphere, then the assumption is valid.
\subsection{Directional Derivatives and the Gradient Vector}
\subsubsection{The Directional Derivative}
Consider a differentiable function \(f(x,y)\). Recall that \(f_x\) and \(f_y\) gave us the rate of change of \(f\) in the \(+x\) and \(+y\) directions respectively. Now, we will introduce a method to get the rates of change of functions in an \it{arbitrary} direction. If a unit vector \(\vec u = \<a, b\>\) is the direction we are trying to find the rate of change in, then the \bf{directional derivative} of \(f\) in the direction of \(\vec u\) tells us this. The \(x\) and \(y\) components of this will be the projection of the vector with components \(\< f_x, f_y\>\) onto \(\vec u\). In equation form,
\[ D_{\vec u} f = af_x + bf_y = \<f_x, f_y\> \cdot\<a, b\> \] 
\begin{definition}
    The \bf{directional derivative} of \(f\) at \((x_0, y_0)\) in the direction of a unit vector \(\vec u - \<a, b\>\) is
    \[ D_{\vec u}f(x_0, y_0) = \lim_{h\to 0}\frac{f(x_0+a, y_0+b) - f(x_0, y_0)}{h}\]
\end{definition}
Note that if \(\vec u = \ihat\), we just get \(f_x\), and if \(\vec u = \jhat\), we just get \(f_y\). This tells us that the partial derivatives of \(f\) are just special cases of the directional derivative. 
\subsubsection{The Gradient Vector}
We will define a new quantity, the gradient, of a function. 
\begin{definition}
    If \(f(x_1, x_2, \dots, x_n)\) is a differentiable function, then the gradient of \(f\) is the vector function \(\nabla f\) defined by
    \[ \nabla f = \<\pdv{f}{x_1}, \pdv{f}{x_2}, \dots, \pdv{f}{x_n}\> \]
\end{definition}
\begin{example}
    Find \(\nabla f\) where \(f(x,y)=\sin x + e^{xy}\).\par\bf{Solution: }Just plug in.
    \begin{align*}
        \nabla f &= \<\pdv{f}{x}, \pdv{f}{y}\> \\
        &= \< \cos x + ye^{xy}, xe^{xy} \>
    \end{align*}
\end{example}
With this new notation, we can rewrite our expression for the directional derivative.
\begin{theorem}
If \(f(x_1, x_2, \dots, x_n)\) is a differentiable function, and \(\vec u = \<u_1, u_2, \dots, u_n\>\) is a unit vector, then the directional derivative of \(f\) in the direction of \(\vec u\) at a point \(\vec t = \<t_0, t_1, \dots, t_n\>\) is given by 
\[ D_{\vec u}f(\vec t) = \nabla f(\vec t)\cdot \vec u\]
\end{theorem}
\begin{proof}
    First, define \(g(a) = f(\vec t + a\vec u)\). Then, we must have \(x_i = t_i + au_i\). Further, by the definition of the derivative, we have
    \begin{align*}
        \dv{g}{a} &= \lim_{h\to 0}\frac{g(a+h)-g(a)}{h} \\
        &= \lim_{h\to 0}\frac{f(\vec t+(a+h)\vec u) - f(\vec t + a\vec u)}{h}
    \end{align*}
    Then, evaluating it at \(a=0\),
    \begin{align*}
        \dv{g}{n}\biggr|_{a=0} &= \lim_{h\to 0}\frac{f(\vec t + h\vec u)-f(\vec t)}{h}
    \end{align*}
    This definition may look similar. It is the same as the definition of the directional derivative. That is,
    \begin{equation}\label{directional}
     \dv{g}{n}\biggr|_{a=0} = D_{\vec u}(\vec t)
    \end{equation} We can also find the derivative of \(g\) with the chain rule, which tells us
    \begin{align*}
        \dv{g}{a}\biggr|_{a=0} &= \dv{f}{(\vec t + a\vec u)}\biggr|_{a=0}\dv{a}\bqty{t+a\vec u} \\
        &=\dv{f}{(\vec t + a\vec u)}\biggr|_{a=0} = \dv{f}{v}\biggr|_{v=\vec t} = \partial_{x_1}f(\vec t)\dv{x_1}{a} +\partial_{x_2}f(\vec t)\dv{x_2}{a} + \cdots + \partial_{x_n}f(\vec t)\dv{x_n}{a} 
    \end{align*}
    Recall that \(x_i = t_1+au_1\), so \(\dd x_i/\dd a = u_i\). Therefore,
    \begin{align*}
        \dv{g}{a}\biggr|_{a=0} &= \partial_{x_1}f(\vec t)u_1 + \partial_{x_2}f(\vec t)u_2 + \cdots + \partial_{\vec t}f(t_n){x_n}u_n \\
        &= \< \partial_{x_1}f(\vec t), \partial_{x_2}f(\vec t), \dots, \partial_{x_n}f(\vec t) \> \cdot \< u_1, u_2, \dots, u_n\> \\
        &= \nabla f(\vec t) \cdot \vec u
    \end{align*}
    Finally, equating this result with Eq. \ref{directional}, we get
    \[ D_{\vec u}f(\vec t) = \nabla f(\vec t) \cdot \vec u\]
\end{proof}
\begin{example}
    Find the directional derivative of the function \(f(x,y) = x^2y^3-4y\) at the point \((2, -1)\) in the direction of \(\vec v = \<2, 5\>\).\par\bf{Solution: }First, compute the gradient vector of \(f\) at \((2, -1)\):
    \begin{align*}
        \nabla f &= \<2xy^3, 3x^2y^2-4\> \\
        \nabla f(2, -1) &= \<2(2)(-1)^3, 3(2)^2(-1)^2-4\> \\
        &= \< -4, 8\>
    \end{align*}
    Then, find the unit vector in the direction of \(\vec v\),
    \[ \vec u =\frac{\vec v}{\norm{\vec v}} = \frac{1}{\sqrt{29}}\vec v = \<\frac{2}{\sqrt{29}}, \frac{5}{\sqrt{29}}\>\]
    Finally, compute the directional derivative,
    \begin{align*}
        D_{\vec u}f(2, -1) = \nabla f(2, -1)\cdot \vec u = \<-4, 8\>\cdot \<\frac{2}{\sqrt{29}}, \frac{5}{\sqrt{29}}\> = \frac{32}{\sqrt{29}}
    \end{align*}
\end{example}
\begin{example}
    If \(f(x,y,z)=x\sin yz\), find the gradient of \(f\) and the directional derivative of \(f\) at \((1,3,0)\) in the direction of \(\vec v = \<1, 2, -1\>\).\par\bf{Solution: }First, compute the gradient,
    \begin{align*}
        \nabla f = \<f_x, f_y, f_z\> &= \<\sin yz, zx\cos yz, yx\cos yz\>
    \end{align*}
    Then, find \(\nabla f(1,3,0)\),
    \[ \nabla f(1,3,0) = \<\sin 0, 0\cos 0, 3\cos 0\> = \<0, 0, 3\>\]
    Then, compute \(\vec u = \vec v/\norm{\vec v}\),
    \[ \vec u = \frac{1}{\sqrt 6}\vec v \]
    And find the directional derivative,
    \[ D_{\vec u}f(1,3, 0) = \<0, 0, 3\> \cdot \<\frac{1}{\sqrt 6}, \frac{2}{\sqrt 6}, -\frac{1}{\sqrt 6}\> = -\frac{3}{\sqrt 6}\]
\end{example}
\subsubsection{Maximizing the Directional Derivative}
Suppose we have a function \(f\) of \(n\) variables, and we consider all possible directional derivatives of \(f\) at a given point. In which direction would \(f\) change fastest, and how fast would that be? The answers are provided by the following theorem.
\begin{theorem}
    Suppose \(f\) is a differentiable function of several variables. The maximum value of the directional derivative \(D_{\vec u}f(\vec x)\) is \(\norm{\nabla f(\vec x)}\), and it occurs when \(\vec u\) is in the same direction as \(\nabla f\).
\end{theorem}
\begin{proof}
    Because \(D_{\vec u}f = \nabla f \cdot \vec u\), we can rewrite as 
    \[ D_{\vec u}f = \norm{\nabla f}\hspace{0.25mm}\norm{\vec u}\cos \theta \]
    Where \(\theta\) is the angle between \(\vec u\) and \(\nabla f\). This expression reaches its maximum when \(\cos\theta=1\), or when \(\theta = 0\). That is, when the direction of \(\nabla f\) and \(\vec u\) is the same.\par
    Now, if we let \(\vec u = \frac{\nabla f}{\norm{\nabla f}}\) (the unit vector in the direction of \(\nabla f\)), we can compute the value of the directional derivative,
    \[ D_{\vec u}f = \nabla f \cdot \vec u = \nabla f \cdot \frac{\nabla f}{\norm{\nabla f}} = \frac{\norm{\nabla f}^2}{\norm{\nabla f}} = \norm{\nabla f}\]
\end{proof}
By the same reasoning, we can see that the directional derivative of \(f\) is \(0\) when \(\vec u\) is orthogonal to \(\nabla f\), and that the directional derivative is at its minimum of \(-\norm{\nabla f}\) when \(\nabla f\) and \(\vec u\) are pointing in opposite directions.
\begin{example}
    If \(f(x,y)=xe^y\), find the rate of change of \(f\) at the point \(P(2,0)\) in the direction from \(P\) to \(Q(0.5, 2)\). Then, find the direction where \(f\) has the greatest rate of change, and find the value of the greatest rate of change.\par\bf{Solution:}
    First, find the gradient of \(f\) at \(P\),
    \begin{align*}
        \nabla f &= \<e^y, xe^y\> \\
        \nabla f(2, 0) &= \<1, 2\>
    \end{align*}
    And the vector from \(P\) to \(Q\),
    \begin{align*}
        \overrightarrow{PQ} = \<0.5 - 2, 2 - 0\> &= \<-1.5, 2\> \\
        \vec u = \frac{\overrightarrow{PQ}}{\norm{\overrightarrow{PQ}}} = \<-\frac{3/2}{5/2}, \frac{2}{5/2}\> &= \<-\frac{3}{5}, \frac{4}{5} \>
    \end{align*}
    And the directional derivative,
    \[ D_{\vec u}f(2, 0) = \nabla f(2, 0)\cdot \vec u = -\frac{3}{5}+\frac{8}{5} = 1\]
    To find the maximum rate of change, first normalize the gradient,
    \[ \vec u_2 = \frac{\nabla f(2, 0)}{\norm{\nabla f(2, 0)}} = \< \frac{1}{\sqrt 5}, \frac{2}{\sqrt 5} \> \]
    This is the ``direction of greatest ascent.'' Then, compute the directional derivative,
    \[ D_{\vec u_2}f(2,0) = \norm{\nabla f(2, 0)} = \sqrt 5\]
\end{example}
\begin{example}
    Suppose that the temperature at a point \((x,y,z)\) in space is given by
    \[ T(x,y,z) = \frac{80}{1+x^2+2y^2+3z^2}, \]
    where \(T\) is measured in degrees Celsius and \(x,y,z\) in meters. In which direction does temperature increase the fastest at the point \((1, 1, -2)\), and how fast does it change if you are moving at a speed of \(v\hspace{0.25mm}\unit{\meter \second^{-1}}\)?\par\bf{Solution:}
First, compute the gradient vector.
\begin{align*}
    \nabla T &= \< -\frac{160x}{(1+x^2+2y^2+3z^2)^2}, -\frac{320y}{(1+x^2+2y^2+3z^2)^2}, -\frac{480z}{(1+x^2+2y^2+3z^2)^2} \> \\
    \nabla T(1, 1, -2) &= \<-\frac{160}{(1+1^2+2(1)^2+3(-2)^2)^2}, -\frac{320}{(1+1^2+2(1)^2+3(-2)^2)^2}, \frac{960}{(1+1^2+2(1)^2+3(-2)^2)^2} \> \\
    &= \<-\frac{160}{16^2}, -\frac{320}{16^2}, \frac{960}{16^2}\> = \frac{5}{8}\<-1, -2, 6\>
\end{align*}
This is the direction of greatest ascent. The norm of it is that value of the directional derivative in that direction. \[ \norm{\nabla T(1, 1, -2)} = \frac{5}{8}\sqrt{(-1)^2+(-2)^2+6^2} = \frac{5\sqrt{41}}{8}\]
This can be interpreted as the maximal rate of change of temperature per meter. If you are moving at \(v\) meters per second, the maximum rate of change will be the product of these two numbers. \par
So the maximum rate of increase of temperature is \(5v\sqrt{41}/8\approx 4v\hspace{0.5mm}\unit{\celsius \second^{-1}}\).
\end{example}
\subsubsection{Tangent Planes to Level Surfaces}
Suppose \(S\) is a surface with equation \(F(x_1, x_2, \dots, x_n)=k\), that is, it is a level surface of a function \(F\) of \(n\) variables. Let \(P(p_1, p_2, \dots, p_n)\) be a point on \(S\), and let \(C\) be any curve that lies on \(S\) and passes through \(P\). \(C\) can be parameterized by some vector function \(\vec r(t) = \<r_1(t), r_2(t), \dots, r_n(t)\>\). Since \(C\) lies on \(S\), any point \(\vec r\) must satisfy the equation 
\[ F(\vec r) = k \]
If \(r_1\), \(r_2\), and so on are all differentiable functions of \(t\) and \(F\) is also differentiable, we can use the chain rule to differentiate both sides with respect to \(t\), 
\[ \pdv{F}{r_1}\dv{r_1}{t} + \pdv{F}{r_2}\dv{r_2}{t} + \dots + \pdv{F}{r_n}\dv{r_n}{t} = 0 \]
Which can be rewritten as
\[ \nabla F(\vec r(t)) \cdot \vec r'(t) = 0 \]
We can interpret this to say that the gradient vector at any point on a surface is orthogonal to the tangent vector of the curves that lies on that surface and pass through \(P\). In this case, it tells us that the gradient vector of a surface is orthogonal to the tangent vector to level curve at a point shared between them. We can then define the \bf{tangent plane to the level surface \(F(\vec x)=k\) at \(P(\vec p)\)} as the plane that passes through \(P\) and has the normal vector \(\nabla F(\vec x)\). In equation form,
\[ F_{x_1}(\vec x)(x_1 - p_1) + F_{x_2}(\vec x)(x_2 - p_2) + \cdots + F_{x_n}(\vec x)(x_n - p_n) = 0\]
The \bf{normal line} to \(S\) at \(P\) is the line that passes through \(P\) and is orthogonal to the tangent plane. So the direction of this line is given by \(\nabla F(\vec x)\) and has symmetric equations
\[ \frac{x_1 - p_1}{F_{x_1}(\vec x)} = \frac{x_2 - p_2}{F_{x_2}(\vec x)} = \cdots = \frac{x_n - p_n}{F_{x_n}(\vec x)} \]
\subsubsection{Gradient Descent}
Gradient descent is most well known in the context of machine learning applications, but it is a purely mathematical method of optimization at its core. Consider some multivariable function \(f(\vec x)\) and a particular point \(\vec a_0\) on the domain of \(f\). If we want to minimize the value of \(f\), one way of doing that is via the method of gradient descent. \par First, define some positive constant \(\gamma\), which is called the \it{learning rate}. Then, because we know that the direction of fastest descent is in the direction of \(-\nabla f\), we can let \(\vec a_1 = \vec a_0 - \gamma \nabla f(\vec a_0)\). Subtracting the \(\gamma \nabla f(\vec a_0)\) term will move us towards a local minimum of \(f\). If we run this process repeatedly, we will (hopefully) converge to some local minimum. This will not always occur, due to potentially overshooting the minimum value with our step. \par The nuances of picking an ideal learning rate (or an adaptable rate, in some cases) is more fit for a higher-level computer science class, but the basic ideas still apply.\par To connect this to machine learning, consider some function \(L(\vec x)\), where \(L\) outputs the loss (error) of a layer in a neural network, and \(\vec x\) represents the ``settings'' of that layer. Gradient descent will bring us towards the local minimums of \(L\), and will (hopefully) give the ideal, or close to ideal, settings vector \(\vec x\) to minimize the loss \(L(\vec x)\).
\subsection{Maximum and Minimum Values}
One of the primary applications of single-variable calculus is finding the maximum and minimum values of functions in \(\RR^2\). With the tools we've learned since then, we are now able to find the maximum and minimum values of multivariable functions.
\begin{definition}
    A continuous function \(f: \RR^n\to \RR\) has a \bf{local maximum} at \(\vec a\) if \(f(\vec x) \leq f(\vec a)\) for all \(\vec x\) satisfying \(\norm{\vec x - \vec a} \leq \delta\) for some \(\delta > 0\). \par
    \(f\) has a \bf{local minimum} at \(\vec b\) if \(f(\vec x)\geq f(\vec b)\) for all \(\vec x\) satisfying \(\norm{\vec x - \vec b} \leq \delta\) for some \(\delta > 0\).
\end{definition}
This definition can be interpreted as saying that \(\vec a\) and \(\vec b\) give the largest and smallest values of \(f\) for some disk/sphere/hypersphere around them.\par Another theorem will also come in handy.
\begin{theorem}
    If \(f\) has a local maximum or minimum at \(\vec a\) and the first order partial derivatives of \(f\) exist there, then \(\nabla f = \vec 0\).
\end{theorem}
\begin{proof}
    Let \(g_i(x_i) = f(a_1, a_2, \dots, x_i, \dots, a_n)\) for all \(1\leq i\leq n\). If \(f\) has a local extremum at \(\vec x = \vec a\), then \(g_i\) must as well at \(x_i=a_i\), so \(g_i'(a_i)=0\). We can differentiate \(g_i\) to find that \(g_i'(x_i) = f_{x_i}(a_1, a_2, \dots, x_i, \dots, a_n)\). Then, \(g_i'(a_i)=0\), and therefore \(f_{x_i}(\vec a) = 0\) for all integers \(i\) on \([1, n]\). In other words, \(\nabla f = 0\).
\end{proof}
Let's consider the implications of this theorem on the tangent plane to surfaces in \(\RR^3\). If \(f: \RR^2\to \RR\) has a local extremum at \((a, b)\), then \(f_x(a,b)=f_y(a,b)=0\). Then, the tangent plane to \(f\) at \((a,b)\),
\[ f_x(a,b)(x - a) + f_y(a,b)(y-a) + z-f(a, b) \]
can be rewritten as
\[ z = f(a,b) \]
This plane will be flat in the \(xy\) plane, which matches with our intuition for what a local extremum should look like (similar to how the tangent line to local extrema in single-variable calculus is a horizontal line).\par
A point \(\vec x\) is called a \bf{critical point} (or \it{stationary point}) of \(f\) if \(\nabla f(\vec x) = \vec 0\) or if any of the partial derivatives of \(f\) does not exist.
\begin{example}
    Let \(f(x,y)=x^2+y^2-2x-6y+14\). Then,
    \[ f_x(x,y) = 2x-2\quad\text{and}\quad f_y(x,y) = 2y-6\]
    These partial derivatives are only both zero at \((x,y) = (1, 3)\), so that is the only critical point of \(f\). Further, we can complete the square to rewrite \(f\) as 
    \[ f(x,y) = (x-1)^2 + (y-3)^2+4 \]
    because \((x-1)^2\) and \((y-3)^2\) are both strictly increasing as \(x\) and \(y\) get further from \((1, 3)\), this point is a local minimum. In fact, this will be the absolute minimum of \(f\), which turns out to be an elliptic paraboloid with vertex \((1, 3, 4)\).
\end{example}
\begin{example}
    Find the extreme values of \(f(x,y) = y^2-x^2\).\par\bf{Solution}
    Because \(f_x = -2x\) and \(f_y = 2y\), the only critical point is \((x,y)=(0,0)\). Notice that if we affix \(x=0\), \(f\) is strictly increasing with \(y\) getting further from \(0\). However, affixing \(y=0\) shows us that \(f\) is strictly decreasing with \(x\) getting further from \(0\). This type of point is neither an absolute minimum nor maximum. In fact, it will be a \it{saddle point}. 
\end{example}
Saddle points, named after their resemblance to saddles, are any stationary points that are not local extrema.\par
In order to quickly determine what points are relative minima, maxima, or saddle points, we can use an extended version of the second derivative test from single-variable calculus.
\begin{theorem}
    Suppose the second partial derivatives of some function \(f: \RR^n \to \RR\) are continuous on a hypersphere with center \(\vec x\), and suppose that \(\nabla f = \vec 0\). Let 
    \[ H_f(\vec x) = \begin{bmatrix}
        f_{x_1x_1}(\vec x) & f_{x_1x_2}(\vec x) & \cdots & f_{x_1x_n}(\vec x) \\
        f_{x_2x_1}(\vec x) & f_{x_2x_2}(\vec x) & \cdots & f_{x_2x_n}(\vec x) \\
        \vdots & \vdots & \ddots & \vdots \\
        f_{x_nx_1}(\vec x) & f_{x_nx_2}(\vec x) & \cdots & f_{x_nx_n}(\vec x)
    \end{bmatrix}\]
    This matrix is known as the \it{hessian matrix}. Then,
    \begin{enumerate}
        \item If \(H_f(\vec x)\) is positive definite (has all positive eigenvalues), then \(f\) has a local minimum at \(\vec x\).
        \item If \(H_f(\vec x)\) is negative definite (has all negative eigenvalues), then \(f\) has a local maximum at \(\vec x\).
        \item If \(H_f(\vec x)\) has both positive and negative eigenvalues, then \(f\) has a saddle point at \(\vec x\).
    \end{enumerate}
    If none of these conditions are met, the test fails, and we must use another method.\par
    In the special case of two variables, the hessian becomes 
    \[ H_f(a, b) = \begin{bmatrix}
        f_{xx}(a, b) & f_{xy}(a,b) \\
        f_{yx}(a,b) & f_{yy}(a,b)
    \end{bmatrix}\]
    and the conditions simplify. Define \(D=\det H_f(a,b)\). Then,
    \begin{enumerate}
        \item If \(D > 0\) and \(f_{xx} > 0\), \(f\) has a local minimum at \((a, b)\).
        \item If \(D > 0\) and \(f_{xx} < 0\), \(f\) has a local maximum at \((a,b)\).
        \item If \(D < 0\), \(f\) has a saddle point at \((a,b)\).
    \end{enumerate}
    If none of these conditions are met, the test fails, and we must use another method.
\end{theorem}
Now, let's justify the connection between these two. First, assume \(f\) and all of \(f\)'s first partial derivatives are differentiable. Then, compute the eigenvalues of \(H\):
\begin{align*}
    \det (H_f - \mathbb{I}_2\lambda) &= \begin{vmatrix}
        f_{xx} - \lambda & f_{xy} \\
        f_{yx} & f_{yy} - \lambda
    \end{vmatrix} \\
    &= \lambda^2 - \lambda (f_{xx} + f_{yy}) - f_{xy}^2 + f_{xx}f_{yy}
\end{align*}
The quadratic formula tells us 
\begin{align*}
    \lambda &= \frac{f_{xx}+f_{yy}\pm \sqrt{(f_{xx}+f_{yy})^2-4(-f_{xy}^2+f_{xx}f_{yy})}}{2} \\
    2\lambda &= f_{xx}+f_{yy} \pm \sqrt{(f_{xx}+f_{yy})^2-4(f_{xx}f_{yy}-f_{xy}^2)}
\end{align*}    
If \(D>0\), we can guarantee that \(f_{xx}f_{yy} - f_{xy}^2 > 0\). This gives us two results. First, we know that \(f_{xx}\) and \(f_{yy}\) are must have the same sign, because the product \(f_{xx}f_{yy}\) must be positive. Then, we can also rewrite
\[ 2\lambda = f_{xx} + f_{yy} \pm \sqrt{(f_{xx}+f_{yy})^2-c} \]
For some \it{positive} constant \(c\). Note that
\begin{equation}\label{comparison-1}
    \abs{f_{xx}+f_{yy}} > \sqrt{(f_{xx}+f_{yy})^2-c}
\end{equation}
for any positive \(c\). Therefore, in the first case where \(f_{xx}\) and \(f_{yy}\) are positive, 
we have 
\[ 2\lambda = f_{xx} + f_{yy} \pm \sqrt{(f_{xx}+f_{yy})^2-c} \]
The right side is guaranteed to be positive for both solutions, because the \(\sqrt{(f_{xx}+f_{yy})^2-c}\) term cannot subtract more than the \(f_{xx}+f_{yy}\) term adds, as per Eq. \ref{comparison-1}. \par
Similarly, in the case where \(f_{xx}\) and \(f_{yy}\) are negative, we have
\[ 2\lambda = -\abs{f_{xx}} - \abs{f_{yy}} \pm \sqrt{(f_{xx}+f_{yy})^2-c}\]
The right side is guaranteed to be negative for both solutions, because the \(\sqrt{(f_{xx}+f_{yy})^2-c}\) term cannot add more than the \(-\abs{f_{xx}}-\abs{f_{yy}}\) term subtracts, as per Eq. \ref{comparison-1}.\par
Therefore, if \(D>0\) and \(f_{xx} > 0\), both eigenvalues must be positive. If \(D>0\) and \(f_{xx} < 0\), both eigenvalues must be negative.
\begin{example}
    Find the local maximum and minimum values and saddle points of \(f(x,y)=x^4+y^4-4xy+1\).\par\bf{Solution: }
    First, compute each partial:
    \[ f_x = 4x^3-4y\quad\text{and}\quad f_y = 4y^3-4x\]
    Then, setting these equal to zero, we get
    \[ x^3-y=0\quad\text{and}\quad y^3-x=0\]
    The first equation can be rearranged to say \(y=x^3\), which can then be plugged into the second equation,
    \begin{align*}
        0 &= (x^3)^3 -x = x^9 -x \\&= x(x^8 - 1) = x(x^4-1)(x^4+1) \\
        &= x(x^2-1)(x^2+1)(x^4+1)
    \end{align*}
    So our roots are \(x=-1, 0, 1\), which give us the three critical points \((-1, -1)\), \((0, 0)\), and \((1, 1)\). Now, let's calculate the second partials and hessian determinant,
    \begin{align*}
        f_{xx} = 12x^2\quad f_{yy} = 12y^2\quad f_{xy}=-4 \\
        D = f_{xx}f_{yy} - f_{xy}^2 = 144x^2y^2-16
    \end{align*}
    Then check each of the three points:
    \begin{enumerate}
        \item \((-1, -1)\): \(D(-1, -1) = 128\), \(f_{xx}(-1, -1) = 12\). So \((-1, -1)\) is a local minimum.
        \item \((0, 0)\): \(D(0, 0) = -16\). So \((0, 0)\) is a saddle point.
        \item \((1, 1)\): \(D(1, 1) = 128\), \(f_{xx}(1, 1)= 12\). So \((1, 1)\) is a local minimum.
    \end{enumerate}
\end{example}
\begin{example}
    Find and identify the critical points of the function
    \[ f(x, y) = 10x^2y-5x^2-4y^2-x^4-2y^4 \]\bf{Solution: }First, taking each partial,
    \[ f_x = 20xy - 10x - 4x^3\quad f_y = 10x^2-8y-8y^3\]
    And setting them equal to zero,
    \[ 20xy - 10x -4x^3 = 0\quad 10x^2-8y-8y^3 = 0\]
    We can immediately see one solution, \(x=y=0\). To find the others, we can start by solving for \(y\) in the first equation,
    \[ y = \frac{4x^3+10x}{20x} = \frac{2x^2+5}{10}\]
    and then substituting it back into the second one,
    \begin{align*}
        10x^2-8\bqty{\frac{2x^2+5}{10}}-8\bqty{\frac{2x^2+5}{10}}^3 &= 0
    \end{align*}
    Solving by calculator, we find the solutions to be \(x = \pm 0.857, \pm 2.644\). Then, substitute that back into the equation for \(y\) and find the points to be \((-2.644, 1.898)\), \((-0.857, 0.647)\), \((0.857, 0.647)\), and \((-2.644, 1.898)\).\par Now, taking the second partials,
    \[ f_{xx} = 20y-10 - 12x^2\quad f_{yy} = -8-24y^2\quad f_{xy} = 20x \]
    and calculating the hessian determinant,
    \[ D = f_{xx}f_{yy}-f_{xy}^2 = (-12x^2+20y-10)(-8-24y^2) - (20x)^2\]
    \begin{enumerate}
        \item \((0,0)\): \(D=80\), and \(f_{xx} = -10\), so it is a relative maximum. 
        \item \((\pm2.644, 1.898)\):  \(D\approx 2486.614\), and \(f_{xx}\approx -55.929\), so they are relative maximums
        \item \((\pm0.857, 0.647)\): \(D\approx -187.785\), so they are saddle points.
    \end{enumerate}
\end{example}
\begin{example}
    Find the shortest distance from the point \(P(1, 0, -2)\) to the plane \(x+2y+z=4\).\par\bf{Solution: }
    First, because \(P\) does not satisfy the equation \(x+2y+z=4\), it isn't on the plane. Now, let \(Q(x,y,z)\) be a point on the plane. Then, the distance between \(Q\) and \(P\) is given by
    \[ \ell = \sqrt{(x-1)^2 + y^2 + (z+2)^2} \]
    We can rewrite the equation of the plane as \(z=4-x-2y\), and substitute that for \(z\) in the equation for \(\ell\):
    \[ \ell = \sqrt{(x-1)^2+y^2+(6-x-2y)^2}\] Now, to find the shortest value of \(\ell\), we need to find its absolute minimum. Our domain is \(\RR^3\), which normally presents an issue because it is an unbounded set (and thus does not always have absolute extrema). However, in this case, we can use our intuition for how distance works to say that there will definitely be an absolute minimum.\par
    To find critical points, we can first redefine \(f=\ell^2 = (x-1)^2+y^2+(6-x-2y)^2\). \(f\) will have the same critical points, but is simpler to differentiate.
    \[ f_x = 2(x-1)-2(6-x-2y) = 4x+4y-14  \]
    \[ f_y = 2y-4(6-x-2y) = 10y + 4x-24\]
    These are both zero when the equations \(10y+4x-24=0\) and \(4x+4y-14=0\) are both satisfied. Rewriting this as a matrix,
    \[ \begin{bmatrix}
        4 & 4 \\ 4 & 10
    \end{bmatrix}\begin{bmatrix}
        x\\y
    \end{bmatrix} = \begin{bmatrix}
        14 \\ 24
    \end{bmatrix}\]
    Which we can solve by row reduction or inverse matrices to find that the only critical point is \(\pqty{\frac{11}{6}, \frac{5}{3}}\). Now, we can compute each second partial,
    \begin{align*}
        f_{xx} = 4 \quad f_{yy} = 10 \quad f_{xy} = 4
    \end{align*}
    So the hessian determinant \(f_{xx}f_{yy}-f_{xy}^2 = 26 > 0\) and \(f_{xx}>0\), so we have a local minimum, which also happens to be the absolute minimum.\par To find the value of the smallest distance, simply plug in the point \(\pqty{\frac{11}{6}, \frac{5}{3}}\) into \(\ell\),
    \[ \ell = \sqrt{\pqty{\frac{11}{6}-1}^2+\pqty{\frac{5}{3}}^2+\pqty{6-\frac{11}{6}-\frac{10}{3}}^2} = \frac{5\sqrt6}{6}\]
\end{example}
\begin{example}
    A rectangular box with no lid is to be made from \(12\) \( \unit{\meter^2}\) of cardboard. Find the maximum volume of such a box.
    \par\bf{Solution: }
    Let \(x\) be the length of the box, \(y\) be the width of the box, and \(z\) be the height of the box. Then, the volume is given by \(V=xyz\) and the surface area is given by \(S = xy+2xz+2yz\). This must be equal to \(12\), so
    \[ xy+2xz+2yz = 12\]
    Which can be rearranged to say
    \[ z = \frac{12-xy}{2x+2y}\]
    Plugging this back into the expression for \(V\),
    \[ V = xy\bqty{\frac{12-xy}{2x+2y}} = \frac{12xy-x^2y^2}{2x+2y}\]
    We can now find the partials of this. 
    \begin{align*}
        \pdv{V}{x} &= \frac{(2x+2y)(12y-2xy^2)-(12xy-x^2y^2)(2)}{(2x+2y)^2}\\& = \frac{24xy+24y^2-4x^2y^2-4xy^3-24xy+2x^2y^2}{(2x+2y)^2} \\
        &= \frac{-4xy^3-2x^2y^2+24y^2}{(2x+2y)^2} = \frac{-2y^2(x^2+2xy-12)}{(2x+2y)^2} \\
        \pdv{V}{y} &= \frac{(2x+2y)(12x-2yx^2)-(12xy-x^2y^2)(2)}{(2x+2y)^2} \\
        &= \frac{24x^2+24xy-4x^3y-4x^2y^2-24xy+2x^2y^2}{(2x+2y)^2} \\
        &= \frac{-4x^3y - 2x^2y^2 + 24x^2 }{(2x+2y)^2} = \frac{-2x^2(y^2+2xy-12)}{(2x+2y)^2}
    \end{align*}
    The first equation is zero when \(y=0\) or \(x^2+2xy-12=0\). The second equation is zero when \(x=0\) or \(y^2+2xy-12=0\). Notice that these two equations imply \(x=y\), so we can substitute that in, and say
    \[ x^2+2x^2-12=0 \implies x=y=2\]
    Note that \(x=y=0\) is not a valid solution because \(V_x\) and \(V_y\) are not continuous there. Then, we can find the \(z\),
    \[ z=\frac{12-xy}{2x+2y} = 1 \]
    We can pretty reasonably assume that this will be an absolute maximum due to the physical implication of this problem.
\end{example}
\subsection{Absolute Minimum and Maximum Values}
Previously, we've been kind of handwaving the issue of finding absolute extrema by using physical interpretations of problems. Now, we will introduce a mathematical way of finding the absolute extrema of functions.\par
First, we will introduce a multivariable equivalent of the extreme value theorem from single-variable calculus.
\begin{theorem}[Extreme Value Theorem]
    If \(f:\RR^n\to \RR\) is continuous on a closed, bounded set \(D\subseteq \RR^n\), \(f\) must have some absolute maximum and minimum value on \(D\). 
\end{theorem}
To find these extreme values, we must check not only the critical values of \(f\), but also all points on the boundary of \(D\). We can write this boundary of \(D\) as the set \(\partial D\).
\begin{definition}
    If \(X\) is a topological space, and \(S\subseteq X\) is a set, then the \bf{boundary set} \(\partial S\) is defined by
    \[ \partial S := \{p\in X: \text{for every neighborhood \(O\) of \(p\)}, O\cap S\neq \emptyset, O\cap(X\setminus S)\neq \emptyset\} \]
\end{definition}
This can be understood to say that the boundary set is the set of all elements that are infinitely close to an element of \(S\) and also infinitely close to an element of \(X\) that is not in \(S\). \par
\begin{example}
    Find the absolute maximum and minimum values of \(f(x,y)=x^2-2xy+2y\) on the rectangle \(D=\{(x,y)|0\leq x\leq 3, 0\leq y\leq 2\}\).
    \par\bf{Solution: }First, let's find the partials of \(f\),
    \[ f_x = 2x-2y \quad f_y = -2x+2 \]
    So \((x,y) = (1, 1)\) is the only critical point, which will have a value of \(f(1, 1) = 1\). We can break up the boundary set \(\partial D\) into four line segments. 
    \begin{enumerate}
        \item \(x=0\) and \(y\in[0, 2]\)
        \item \(x=3\) and \(y\in[0,2]\)
        \item \(y=0\) and \(x\in[0,3]\)
        \item \(y=2\) and \(x\in[0,3]\)
    \end{enumerate}
    Describing the behavior of \(f\) on these segments, we get
    \begin{enumerate}
        \item \(f(0, y) = 2y\), \(y\in[0, 2]\)
        \item \(f(3, y) = 9 - 4y\), \(y\in[0,2]\)
        \item \(f(x,0) = x^2\), \(x\in[0, 3]\)
        \item \(f(x, 2) = x^2-4x+4\), \(x\in[0, 3]\)
    \end{enumerate}
The absolute maximums of these four segments are \(f(0, 2) = 4\), \(f(3, 0) = 9\), \(f(3, 0) = 9\), and \(f(0,2) = 4\). \par
The absolute minimums of these four segments are \(f(0, 0)=0\), \(f(3, 2)=1\), \(f(0,0)=0\), and \(f(2, 2)=0\). 

\par Therefore, the absolute minimum of \(f\) is the smallest value of \(f\) of these \(10\), which will be \(f(0, 0) = 0\). Similarly, the absolute maximum of \(f\) is the largest value of \(f\) of these \(10\), which will be \(f(3, 0)=9\).
\end{example}
\subsection{Lagrange Multipliers}
Lagrange multipliers give us an easier way to maximize or minimize a function subject to a constraint.\par
Consider some function \(f(x,y,z)\) and some constraint \(g(x,y,z)=k\). Geometrically, this is like trying to find the maximum value of \(f\) on the level surface described by \(g(x,y,z)=k\). This can also be interepreted as finding the largest value \(c\) such that the level surfaec \(f(x,y,z)=c\) intersects the level surface \(g(x,y,z)=k\). We can reason that if we have reaches the highest \(c\)-value, then the two surfaces will be tangent to each other. Otherwise, \(c\) could be increased further. If these two surfaces are tangent, then their normal lines are identical at the point of tangency. Therefore, their gradient vectors must be parallel, which gives us the equation \(\nabla f = \lambda \nabla g\) for some scalar \(\lambda\). \par
The \(\lambda\) in this equation is known as the \bf{lagrange multiplier}. The procedure to use the lagrange multiplier is as follows: \par
First, find all values of \(\vec x\), and \(\lambda\) such that
\[ \nabla f(\vec x) = \lambda\nabla g(\vec x) \quad\text{and}\quad g(\vec x)=k\]
Then, evaluate \(f\) at each of these points. The largest of these values is the absolute maximum of \(f\) constrained by \(g\), and the smallest of these values is the absolute minimum of \(f\) constrained by \(g\).
\par
In the special case of functions of three variables, we can write \(\nabla f = \lambda\nabla g\) as a system of four equations
\[ f_x = \lambda g_x,\quad f_y=\lambda g_y,\quad f_z=\lambda g_z,\quad g(x,y,z)=k\]
\begin{example}
    We will repeat an example from earlier, but now using the method of lagrange multipliers.\par If a box with no top is to be made from \(12\) \( \unit{\meter^2} \) of cardboard, find the maximum volume.\par\bf{Solution: }First, write the equations \(V = xyz\) and \(S = 12=xy+2xz+2yz\). Our constraint is \(S\) and our function is \(V\), so we have
    \[ \nabla V = \lambda \nabla S\]
    which becomes
    \begin{equation}\label{system} \begin{bmatrix}
        yz \\ xz \\ xy
    \end{bmatrix} = \lambda \begin{bmatrix}
        y + 2z \\ x + 2z \\ 2x + 2y
    \end{bmatrix}\end{equation}
    This, along with \(S=12\), gives us a system of four equations. There are no general rules for solving systems of (nonlinear) equations such as this one, so we might have to have some ingenuity. First, note that we can modify equation \ref{system} to say
    \begin{align*}
        xyz &= \lambda (xy + 2xz) \\
        xyz &= \lambda (xy+ 2yz) \\
        xyz &= \lambda(2xz+2yz)
    \end{align*}
    Note that \(xy+2xz+2yz\) necessitates that \(\lambda \neq 0\) because that would cause \(x=y=z=0\). \par
    Now, we can see from the first and second of the above equations that
    \[ xy+2xz = xy+2yz\]
    which tells us that \(xz=yz\), which implies \(x=y\) since \(z\neq 0\) (since this would give \(V=0\)). Therefore, we can equate the second and third equations in our system to give
    \[ xy+2yz = 2xz+2yz\]
    which we can substitute \(x=y\) in to find
    \[ x^2+2xz = 4xz \]
    and \(x(x-2z)=0\), so \(x=2z\). We throw away \(x=0\) because that would give \(V=0\). So we can take our constraint \(xy+2xz+2yz\) and substitute \(x=y=2z\) to find
    \[ 12z^2=12\]
    So \(z=1\). Then \(x=y=2z\), so our absolute maximum is given by \((x,y,z) = (2,2,1)\).
\end{example}
\begin{example}
    Find the extrema of \(f(x,y)=x^2+2y^2\) subject to \(x^2+y^2=1\).\par\bf{Solution: }First, write out the lagrange multiplier system:
    \begin{align}
        2x = \lambda 2x \label{ex11}\\
        4y = 2\lambda y \label{ex12}\\
        x^2+y^2=1 \label{ex13}
    \end{align}
    From \ref{ex11}, we can see that \(\lambda =1\) or \(x=0\). From \ref{ex12}, we can see that \(\lambda=2\) or \(y=0\). From \ref{ex13}, we see that if \(x=0\), then \(y=\pm 1\) and if \(y=0\), then \(y=\pm 1\). So we have four possible points that satisfy the system, \((1, 0)\), \((0, 1)\), \((-1, 0)\), and \((0, -1)\). Plugging each of these into \(f\), we find
    \begin{align*}
        f(1, 0) = 1 \\
        f(0, 1) = 2 \\
        f(-1, 0) = 1 \\
        f(0, -1) = 2
    \end{align*}
    So our two absolute maxima are at \((x,y) = (0, \pm 1)\) and our two absolute minima are at \((x,y)=(\pm 1, 0)\).
\end{example}
\begin{example}
    Find the extrema of \(f(x,y)=x^2+2y^2\) subject to \(x^2+y^2 \leq 1\).\par\bf{Solution: }This is similar to the previous problem, but with a slight variation in the constraint. Now, the system becomes
    \begin{align}
        2x = \lambda 2x \label{ex21}\\
        4y = 2\lambda y \label{ex22}\\
        x^2+y^2\leq 1 \label{ex23}
    \end{align}
    Which is now satisfied by the points \(P = \{(x, 0)|0\leq x\leq 1\}\cup\{(0, y)|0\leq y\leq 1\}\). We now want to compare both the points at the boundary of the constraint and all critical points of \(f\).\par Because \(\nabla f = \<2x, 4y\>\), the only critical point is \(f(0, 0) = 0\). The points on the boundary have already been evaluated in the previous example (because checking the boundary is equivalent to changing the constraint to \(x^2+y^2=1\), which we just examined). \par Therefore, the absolute maximum is still \(f(0, \pm 1) = 2\), but the absolute minimum is now \(f(0,0)=0\).
\end{example}
\begin{example}
    Find the points on the sphere \(x^2+y^2+z^2=4\) that are closest and farthest from the point \(P(3, 1, -1)\).
    \par\bf{Solution: }
    Consider some arbitary point \(\vec v = \< x, y, z\>\). Then, the distance from \(\vec v\) to \(P\) is 
    \[ d = \sqrt{(x-3)^2+(y-1)^2+(z+1)^2}\]
    To make the math simpler, let's consider \(f = d^2\), so
    \[ f(x, y, z) = (x-3)^2 + (y-1)^2+(z+1)^2\]
    Now, the gradient of \(f\) is given by
    \[ \nabla f = \< 2(x-3), 2(y-1), 2(z+1) \> \]
    The gradient of \(g\) is
    \[ \nabla g = \< 2x, 2y, 2z \>\]
    So our system of equations is
    \begin{align*}
        x-3 &= \lambda x \\
        y-1 &= \lambda y \\
        z+1 &= \lambda z \\
        x^2+y^2+z^2 &= 4
    \end{align*}
    Rearranging the first three equations, we can equate them by isolating \(\lambda\),
    \[ \frac{x-3}{x} = \frac{y-1}{y} = \frac{z+1}{z} \]
    Which can all be rearranged to say
    \[ 1-\frac{3}{x} = 1-\frac{1}{y} = 1+\frac{1}{z} \]
    Then, we can express \(y\) and \(z\) in terms of \(x\):
    \begin{align*}
        y &= \frac{x}{3} \\
        z &= -\frac{x}{3}
    \end{align*}
    Which we can substistute into equation 4,
    \begin{align*}
        4 &= x^2+\pqty{\frac{x}{3}}^2+\pqty{-\frac{x}{3}}^2 \\
        &= \frac{11}{9}x^2 \\
        x &= \pm\frac{6}{\sqrt{11}}
    \end{align*}
    Which gives us two points, \[\vec r_1 = \<\frac{6}{\sqrt{11}}, \frac{2}{\sqrt{11}}, -\frac{2}{\sqrt{11}}\>\quad\text{and}\quad \vec r_2 = \< -\frac{6}{\sqrt{11}}, -\frac{2}{\sqrt{11}},\frac{2}{\sqrt{11}}\>\]
    Clearly, \(f(\vec r_2)\) will give the greater result, so \(f(\vec r_1)\) is an absolute minimum (shortest distance) and \(f(\vec r_2)\) is an absolute maximum (largest distance).
\end{example}
\subsubsection{Several Constraints}
Suppose we want to find the extrema of a function \(f: \RR^n\to \RR\) subject to \(m\) (assuming \(m<n\)) constraints of the form \(g_i(\vec x) = k_i\), with each \(g_i\) taking an \(n\)-dimensional input and returning a scalar output. Geometrically, this means that we are looking for the minimum and maximum values of \(f\) on the surface formed by the intersection of each of the level surfaces formed by \(g_1(\vec x) = k_1\), \(g_2(\vec x) = k_2\), \(\dots\), \(g_m(\vec x) = k_m\). Let's call the surface formed by all of these intersections \(C\). \par
Suppose that \(f\) has an extreme value at a point \(\vec u\in\RR^n\). We know that \(\nabla f(\vec u)\) must be orthogonal to the level surface \(f(\vec x)=c\), which is tangent to \(C\), so \(\nabla f\) must be orthogonal to \(C\) at \(P\). By the same logic, \(\nabla g_i(\vec u)\) is also orthogonal to \(C\) for all \(i\). Then, \(f\) lies in the \(m\) dimensional subspace with basis elements \(\nabla g_1(\vec x), \nabla g_2(\vec x), \dots, \nabla g_m(\vec x)\), and can be written as a linear combination of them,
\begin{equation}\label{constraints}
    \nabla f(\vec u) = \lambda_1 \nabla g_1(\vec u) + \lambda_2\nabla g_2(\vec u)+\cdots + \lambda_m\nabla g_m(\vec u)
\end{equation}
Where each \(\lambda_i\) is a constant.\par This generates a system of \(n+m\) equations: \(n\) from each component of \ref{constraints}, and \(m\) from the restrictions \(g_i(\vec x) = k_i\). \par
Solving this system gives us the values of \(\vec x\) that are candidates for the extrema of \(f\) subject to the \(m\) restrictions formed by \(g_i=k_i\).